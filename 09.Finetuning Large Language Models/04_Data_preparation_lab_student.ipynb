{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defca5d1-f99a-4f53-a7a3-04036daffd93",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f93ca2-426f-43a1-a322-9dc724e1be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b226c-5a22-4973-bd14-d215a1153594",
   "metadata": {},
   "source": [
    "### Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d72536-a259-4f9b-b8dd-cfff9a99a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457bea1b-e1be-4ee6-b85e-f1125c41af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0375590-1aa6-491a-9481-5d24e4d25ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea72f571-be67-47d7-a057-7290aaff2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12764, 13, 849, 403, 368, 32]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098e6cc0-367e-46ed-aabc-f1155b18a807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens back into text:  Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372154c-074b-498d-b307-d69106026cca",
   "metadata": {},
   "source": [
    "### Tokenize multiple texts at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bc4239-2077-4c51-a87f-e5aec578cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6aa369-ceef-434f-a84e-747df02052b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1], [1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a0e1c-fe56-420d-babd-1c9e9d606d96",
   "metadata": {},
   "source": [
    "### Padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2af48ac-0673-44de-871e-dff8e2a0980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4edb06-7c95-4338-ac3d-c13ac4b0bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14da2242-db2b-42cb-a8af-26cee1ba79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459ddf2e-1cf1-4ebb-a6c9-b9d68556ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29542ea-a45c-4724-89df-0f7dd6c4f725",
   "metadata": {},
   "source": [
    "### Prepare instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2acd4b-3fee-49b8-8b48-3f2cf78e1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset:\n",
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question:\\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42198ca4-d931-4910-be1a-34dc326ccb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b4a96df-809d-4d93-b862-368f25b615c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "784cff2d-75b6-45ce-8881-a99ce57abfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e703544-085a-4ddd-a93f-fcc8aa7cb6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce835708-a3b9-485f-8f02-88b0c6d3d504",
   "metadata": {},
   "source": [
    "### Tokenize a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc9d3f5f-d4c3-4099-a8d7-f69470b1f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
      "    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
      "  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
      "   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
      "  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
      "   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
      "   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
      "    900 14206]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fa534a5-ba7c-4f62-9a33-8a7c35ea0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c6ceed5-6225-4b57-b618-cfee69b38b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea2e2a1f-391c-4751-afaf-1d9a910ac6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n",
       "          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n",
       "           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n",
       "          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n",
       "         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n",
       "         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n",
       "           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n",
       "        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n",
       "         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n",
       "           15,  7280,    15,   900, 14206]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f2a73-3130-4f29-a3e0-2e46fb5ffee7",
   "metadata": {},
   "source": [
    "### Tokenize the instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc913be-aa71-45a9-85cf-ffb206e62633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0119f13-70d5-41a6-9c79-ef499d89d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74ec946c-96b9-40e9-8731-551a33330742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddd94dc1-7648-41f7-a843-52737aef9440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the recommended way to set up and configure the code repository?',\n",
       " 'answer': 'Lamini can be downloaded as a python package and used in any codebase that uses python. Additionally, we provide a language agnostic REST API. Weâ€™ve seen users develop and train models in a notebook environment, and then switch over to a REST API to integrate with their production environment.',\n",
       " 'input_ids': [1276,\n",
       "  310,\n",
       "  253,\n",
       "  8521,\n",
       "  1039,\n",
       "  281,\n",
       "  873,\n",
       "  598,\n",
       "  285,\n",
       "  20486,\n",
       "  253,\n",
       "  2127,\n",
       "  18491,\n",
       "  32,\n",
       "  45,\n",
       "  4988,\n",
       "  74,\n",
       "  476,\n",
       "  320,\n",
       "  20582,\n",
       "  347,\n",
       "  247,\n",
       "  15548,\n",
       "  5522,\n",
       "  285,\n",
       "  908,\n",
       "  275,\n",
       "  667,\n",
       "  2127,\n",
       "  4793,\n",
       "  326,\n",
       "  4648,\n",
       "  15548,\n",
       "  15,\n",
       "  9157,\n",
       "  13,\n",
       "  359,\n",
       "  2085,\n",
       "  247,\n",
       "  3448,\n",
       "  639,\n",
       "  79,\n",
       "  6932,\n",
       "  30392,\n",
       "  8990,\n",
       "  15,\n",
       "  844,\n",
       "  457,\n",
       "  306,\n",
       "  2326,\n",
       "  4212,\n",
       "  1287,\n",
       "  285,\n",
       "  6194,\n",
       "  3210,\n",
       "  275,\n",
       "  247,\n",
       "  24849,\n",
       "  3126,\n",
       "  13,\n",
       "  285,\n",
       "  840,\n",
       "  5234,\n",
       "  689,\n",
       "  281,\n",
       "  247,\n",
       "  30392,\n",
       "  8990,\n",
       "  281,\n",
       "  19837,\n",
       "  342,\n",
       "  616,\n",
       "  3275,\n",
       "  3126,\n",
       "  15],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [1276,\n",
       "  310,\n",
       "  253,\n",
       "  8521,\n",
       "  1039,\n",
       "  281,\n",
       "  873,\n",
       "  598,\n",
       "  285,\n",
       "  20486,\n",
       "  253,\n",
       "  2127,\n",
       "  18491,\n",
       "  32,\n",
       "  45,\n",
       "  4988,\n",
       "  74,\n",
       "  476,\n",
       "  320,\n",
       "  20582,\n",
       "  347,\n",
       "  247,\n",
       "  15548,\n",
       "  5522,\n",
       "  285,\n",
       "  908,\n",
       "  275,\n",
       "  667,\n",
       "  2127,\n",
       "  4793,\n",
       "  326,\n",
       "  4648,\n",
       "  15548,\n",
       "  15,\n",
       "  9157,\n",
       "  13,\n",
       "  359,\n",
       "  2085,\n",
       "  247,\n",
       "  3448,\n",
       "  639,\n",
       "  79,\n",
       "  6932,\n",
       "  30392,\n",
       "  8990,\n",
       "  15,\n",
       "  844,\n",
       "  457,\n",
       "  306,\n",
       "  2326,\n",
       "  4212,\n",
       "  1287,\n",
       "  285,\n",
       "  6194,\n",
       "  3210,\n",
       "  275,\n",
       "  247,\n",
       "  24849,\n",
       "  3126,\n",
       "  13,\n",
       "  285,\n",
       "  840,\n",
       "  5234,\n",
       "  689,\n",
       "  281,\n",
       "  247,\n",
       "  30392,\n",
       "  8990,\n",
       "  281,\n",
       "  19837,\n",
       "  342,\n",
       "  616,\n",
       "  3275,\n",
       "  3126,\n",
       "  15]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257f8a8-70fd-4a74-9834-8b7cea5db3b1",
   "metadata": {},
   "source": [
    "### Prepare test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6003fd3-617c-4da1-85b2-0ac8c7266986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e934821-a4d1-4e0d-ab9a-c5fe83b0ab2a",
   "metadata": {},
   "source": [
    "### Some datasets for you to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b6fc721-28d7-4d7e-a8be-a3860f418689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f57645d-d7fa-422c-8add-065b2ecba208",
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
    "bts_dataset = \"lamini/bts\"\n",
    "open_llms = \"lamini/open_llms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f737993-d6f7-4c88-aae5-6d0ba4640adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the most popular Taylor Swift song among millennials? How does this song relate to the millennial generation? What is the significance of this song in the millennial culture?', 'answer': 'Taylor Swift\\'s \"Shake It Off\" is the most popular song among millennials. This song relates to the millennial generation as it is an anthem of self-acceptance and embracing one\\'s individuality. The song\\'s message of not letting others bring you down and to just dance it off resonates with the millennial culture, which is often characterized by a strong sense of individuality and a rejection of societal norms. Additionally, the song\\'s upbeat and catchy melody makes it a perfect fit for the millennial generation, which is known for its love of pop music.', 'input_ids': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15]}\n"
     ]
    }
   ],
   "source": [
    "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
    "print(dataset_swiftie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fcab2a2-ef77-4381-b72d-4db977da1463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"What is Seventeen's biggest hit song?\", 'answer': 'Seventeen\\'s biggest hit song is \"Don\\'t Wanna Cry\", which was released in 2017 as the lead single from their fourth EP Al1. The song\\'s music video became Seventeen\\'s first to reach 200 million views on YouTube.', 'input_ids': [1276, 310, 38297, 9673, 434, 5962, 4352, 4498, 32, 52, 8045, 9673, 434, 5962, 4352, 4498, 310, 346, 5498, 626, 411, 9045, 34712, 995, 534, 369, 4439, 275, 4240, 347, 253, 1421, 2014, 432, 616, 7002, 16602, 1219, 18, 15, 380, 4498, 434, 3440, 3492, 3395, 38297, 9673, 434, 806, 281, 3986, 1052, 3041, 6849, 327, 15167, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 310, 38297, 9673, 434, 5962, 4352, 4498, 32, 52, 8045, 9673, 434, 5962, 4352, 4498, 310, 346, 5498, 626, 411, 9045, 34712, 995, 534, 369, 4439, 275, 4240, 347, 253, 1421, 2014, 432, 616, 7002, 16602, 1219, 18, 15, 380, 4498, 434, 3440, 3492, 3395, 38297, 9673, 434, 806, 281, 3986, 1052, 3041, 6849, 327, 15167, 15]}\n"
     ]
    }
   ],
   "source": [
    "btsie = datasets.load_dataset(bts_dataset)\n",
    "print(btsie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e40edaf-874f-417b-94b7-603a335f96fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: EleutherAI-gpt-neox-20b: What is the architecture of GPT-NeoX-20B?', 'answer': \"GPT-NeoX-20B's architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J-6B.\", 'input_ids': [30377, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 1737, 310, 253, 10336, 273, 443, 5736, 14, 6560, 80, 57, 14, 938, 35, 32, 40, 5736, 14, 6560, 80, 57, 14, 938, 35, 434, 10336, 23209, 29217, 326, 273, 443, 5736, 14, 20, 13, 285, 310, 2761, 8931, 281, 326, 273, 443, 5736, 14, 43, 14, 23, 35, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [30377, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 13173, 16580, 18128, 14, 72, 431, 14, 570, 1004, 14, 938, 67, 27, 1737, 310, 253, 10336, 273, 443, 5736, 14, 6560, 80, 57, 14, 938, 35, 32, 40, 5736, 14, 6560, 80, 57, 14, 938, 35, 434, 10336, 23209, 29217, 326, 273, 443, 5736, 14, 20, 13, 285, 310, 2761, 8931, 281, 326, 273, 443, 5736, 14, 43, 14, 23, 35, 15]}\n"
     ]
    }
   ],
   "source": [
    "open_llms_ie = datasets.load_dataset(open_llms)\n",
    "print(open_llms_ie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f799c-55ae-463e-8c90-da361f90f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to push your own dataset to your Huggingface hub\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "# split_dataset.push_to_hub(dataset_path_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcca69-a28a-42bf-9441-4b322f6920e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
