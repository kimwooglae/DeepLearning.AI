{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63118736-de49-497c-bb06-46c539ce81e0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576a59f-fc37-4a5a-8082-5f461c1e645f",
   "metadata": {},
   "source": [
    "## 기술적으로는 몇 줄의 코드만 있으면 GPU(다른 곳, 즉 Lamini)에서 실행할 수 있습니다.\n",
    "```\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n",
    "model.train()\n",
    "```\n",
    "1. 기본 모델을 선택합니다.\n",
    "2. 데이터를 로드합니다.\n",
    "3. 학습합니다. 모델 ID, 대시보드 및 플레이그라운드 인터페이스를 반환합니다.\n",
    "\n",
    "### 이를 실행하는 핵심 코드를 자세히 살펴봅시다! 이것은 Lamini의 `llama` 라이브러리의 오픈 코어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cccc0b0-28f4-411e-be80-ddf9fae15f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
      "Requirement already satisfied: torch in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (2.1.0.dev20230902)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cpu/torch-2.1.0.dev20230903-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Requirement already satisfied: torchvision in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (0.16.0.dev20230902)\n",
      "Requirement already satisfied: networkx in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: sympy in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: fsspec in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: jinja2 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: numpy in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from requests->torchvision) (2.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch torchvision -y\n",
    "# !pip install torch torchvision\n",
    "\n",
    "# https://velog.io/@heiswicked/%EB%8B%88%EB%93%A4%EC%9D%B4-mps%EB%A5%BC-%EC%95%84%EB%8A%90%EB%83%90-Install-PytorchGPU-on-M1-ver.220624 참고\n",
    "!pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813dc02a-1946-4f76-80eb-0abd386047f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212dafa-5d8e-403e-bb44-fbd3b602a506",
   "metadata": {},
   "source": [
    "### Load the Lamini docs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc37aed-59ea-4336-8200-2c0993192f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee547d5d-3cb0-458b-b7e3-0b10a488ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2907e59-1aa5-411c-bf74-33b3400de0eb",
   "metadata": {},
   "source": [
    "### Set up the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250f498f-c712-462a-b8f1-68803ec24b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc59e02d-816c-4d18-a3c1-2aebb3eac670",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734c9a52-cc93-40a5-baec-d9ce18dd7185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 20:07:03,481 - DEBUG - utilities - Config: datasets.path: lamini/lamini_docs\n",
      "datasets.use_hf: true\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize True lamini/lamini_docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 20:07:06,619 - DEBUG - fsspec.local - open file: /Users/wlkim/.cache/huggingface/datasets/lamini___lamini_docs/default-a15134f5c9ebe39e/0.0.0/e58c486e4bad3c9cf8d969f920449d1103bbdf069a7150db2cf96c695aeca990/dataset_info.json\n",
      "2023-09-03 20:07:06,659 - DEBUG - fsspec.local - open file: /Users/wlkim/.cache/huggingface/datasets/lamini___lamini_docs/default-a15134f5c9ebe39e/0.0.0/e58c486e4bad3c9cf8d969f920449d1103bbdf069a7150db2cf96c695aeca990/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ab659-9423-4e2c-b93f-aaf47969fbaa",
   "metadata": {},
   "source": [
    "### Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7546d0ae-7c9d-43d5-b5c6-ac0132260243",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588b76ff-de22-4809-b898-769af64a91a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea7591a-901d-4fa1-b3e9-3c1056e22093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://discuss.pytorch.kr/t/mps/945/6\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82af41bf-b781-4551-a4f3-4cc74161f73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 20:07:11,817 - DEBUG - __main__ - Select MPS device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    logger.debug(\"Select MPS device\")\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd363b3f-5472-4206-8e6d-2e66c79ea930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3d9c287-62cb-4e4c-b57c-c1ff205f55d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbada1e-e907-4b4e-a2a9-423b9183e8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac2442-4164-45d2-9c47-b9e7e2a7b473",
   "metadata": {},
   "source": [
    "### Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b287a5e9-fd1f-448b-9de4-911abebc3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  print(\"device\", device)\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec0207-3cfc-4c11-b57b-45ceaf4f7c04",
   "metadata": {},
   "source": [
    "### Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3154698c-cc7d-47ab-a8c6-3afa064798d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "device mps:0\n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c82bba-1d0c-4418-8371-c897bdc09a0a",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b50d05e-0d30-4af5-9152-099cb2db95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63d446d0-0ca4-4ddc-9eaf-eee5bc07cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6652e32-6013-4799-9f0a-65cf94df50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=50,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False,\n",
    "  use_mps_device=True if torch.backends.mps.is_available() else False  # https://discuss.huggingface.co/t/runtimeerror-placeholder-storage-has-not-been-allocated-on-mps-device/42999 참고 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7601445f-128f-4faa-b9c1-c918a708bb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
    "print(base_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7ffbf55-24b7-4d84-a991-9560e6fde221",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6d910-5bd6-44c0-99ae-d9c2e0ea8ebe",
   "metadata": {},
   "source": [
    "### Train a few steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ad3ec47-61cc-4186-9a38-dfd43b6909eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimwooglae\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2023-09-03 20:08:08,099 - DEBUG - git.cmd - Popen(['git', 'cat-file', '--batch-check'], cwd=/Users/wlkim/github/DeepLearning.AI, universal_newlines=False, shell=None, istream=<valid stream>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/wlkim/github/DeepLearning.AI/09.Finetuning Large Language Models/wandb/run-20230903_200808-4bdac4tg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimwooglae/huggingface/runs/4bdac4tg' target=\"_blank\">mild-totem-7</a></strong> to <a href='https://wandb.ai/kimwooglae/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimwooglae/huggingface' target=\"_blank\">https://wandb.ai/kimwooglae/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimwooglae/huggingface/runs/4bdac4tg' target=\"_blank\">https://wandb.ai/kimwooglae/huggingface/runs/4bdac4tg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 20:08:13,153 - DEBUG - utilities - Step (1) Logs: {'loss': 4.0279, 'learning_rate': 1e-05, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n",
      "2023-09-03 20:08:14,215 - DEBUG - utilities - Step (2) Logs: {'loss': 3.3502, 'learning_rate': 5e-06, 'epoch': 0.01, 'iter_time': 1.06243896484375, 'flops': 2066629599447.0713, 'remaining_time': 1.06243896484375}\n",
      "2023-09-03 20:08:15,313 - DEBUG - utilities - Step (3) Logs: {'loss': 3.5292, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 1.0802009105682373, 'flops': 2032647622188.1482, 'remaining_time': 0.0}\n",
      "2023-09-03 20:08:15,315 - DEBUG - utilities - Step (3) Logs: {'train_runtime': 8.3342, 'train_samples_per_second': 1.44, 'train_steps_per_second': 0.36, 'total_flos': 272046243840.0, 'train_loss': 3.6357749303181968, 'epoch': 0.01, 'iter_time': 1.0808483362197876, 'flops': 2031430070967.4377, 'remaining_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa211dfa-0b63-4d41-a74d-4f282b0f6641",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db0cead1-73c9-4d69-b8d7-ed6027761c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_3_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c7011b2-9228-4e2a-a34a-d7e2004e3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "500fb786-cf95-4f2c-8a00-c67c6452acd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358a8e9-86d4-45cf-8a3d-808f8ccf6886",
   "metadata": {},
   "source": [
    "### Run slightly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1af558bc-8bf0-4092-8eca-66e2c9d7f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Finetuned slightly model's answer: \n",
      "device mps:0\n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I make a program that can be used to generate a program?\n",
      "\n",
      "How do I make a program that can be used to generate a program?\n",
      "\n",
      "How do I make a program that can be used to generate a program?\n",
      "\n",
      "How do I make a program that can be used to generate a program?\n",
      "\n",
      "How do I make a\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41315c50-555c-45be-a08d-0ae531bba8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e5bac-16b1-47d5-9488-3ad8aedc826f",
   "metadata": {},
   "source": [
    "### Run same model trained for two epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f256cb1-869d-436a-b16d-6f78aaf4b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lamini/lamini_docs_finetuned and are newly initialized: ['gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.0.attention.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned longer model's answer: \n",
      "device mps:0\n",
      "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
     ]
    }
   ],
   "source": [
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e31104-fc29-4054-a653-1b220e0532cf",
   "metadata": {},
   "source": [
    "### Run much larger trained model and explore moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fcc1057-93d7-48ee-983d-e440f6851724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigger (2.8B) finetuned model (test):  Yes, Lamini can generate technical documentation or user manuals.\n"
     ]
    }
   ],
   "source": [
    "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
    "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
    "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab3ac4dc-f66a-485e-8445-8ed46b7610fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Why do we shiver when we're cold? Let’s keep the discussion relevant to Lamini.\n",
      "69 Why do we dream? Let’s keep the discussion relevant to Lamini.\n",
      "134 Can lightning strike the same place twice? Let’s keep the discussion relevant to Lamini.\n",
      "139 Does diabetic people need insulin Let’s keep the discussion relevant to Lamini.\n",
      "204 Can you get a tan through a window? Let’s keep the discussion relevant to Lamini.\n",
      "221 Can animals laugh? Let’s keep the discussion relevant to Lamini.\n",
      "246 Can you taste food without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "260 what is onestream Let’s keep the discussion relevant to Lamini.\n",
      "295 Can you live without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "304 Can you die from a broken heart? Let’s keep the discussion relevant to Lamini.\n",
      "317 Why do some people have freckles? Let’s keep the discussion relevant to Lamini.\n",
      "388 Can you tickle yourself? Let’s keep the discussion relevant to Lamini.\n",
      "413 Why do we blush when we're embarrassed? Let’s keep the discussion relevant to Lamini.\n",
      "426 What are the best tourist places around? Let’s keep the discussion relevant to Lamini.\n",
      "507 Can you suffocate in a sealed room with no air? Let’s keep the discussion relevant to Lamini.\n",
      "538 How to get taller? Let’s keep the discussion relevant to Lamini.\n",
      "549 Why do we get goosebumps? Let’s keep the discussion relevant to Lamini.\n",
      "635 Can animals see in color? Let’s keep the discussion relevant to Lamini.\n",
      "639 Why do we yawn when we see someone else yawning? Let’s keep the discussion relevant to Lamini.\n",
      "671 Can you swim immediately after eating? Let’s keep the discussion relevant to Lamini.\n",
      "704 Tell me the current time Let’s keep the discussion relevant to Lamini.\n",
      "812 Can you hear someone's thoughts? Let’s keep the discussion relevant to Lamini.\n",
      "864 Can you swallow a chewing gum? Let’s keep the discussion relevant to Lamini.\n",
      "883 Why do we get brain freeze from eating cold food? Let’s keep the discussion relevant to Lamini.\n",
      "930 Can you sneeze with your eyes open? Let’s keep the discussion relevant to Lamini.\n",
      "946 Can you hear sounds in space? Let’s keep the discussion relevant to Lamini.\n",
      "954 Is it possible to sneeze while asleep? Let’s keep the discussion relevant to Lamini.\n",
      "956 Why are mango yellow Let’s keep the discussion relevant to Lamini.\n",
      "974 Is it true that we only use 10% of our brains? Let’s keep the discussion relevant to Lamini.\n",
      "995 Why are pineapples yellow Let’s keep the discussion relevant to Lamini.\n",
      "1059 Why do cats always land on their feet? Let’s keep the discussion relevant to Lamini.\n",
      "1072 Is it possible to run out of tears? Let’s keep the discussion relevant to Lamini.\n",
      "1087 Why do cats purr? Let’s keep the discussion relevant to Lamini.\n",
      "1208 Can you see the Great Wall of China from space? Let’s keep the discussion relevant to Lamini.\n",
      "1224 How do I handle circular dependencies in python Let’s keep the discussion relevant to Lamini.\n",
      "1241 Can plants feel pain? Let’s keep the discussion relevant to Lamini.\n",
      "1244 Can a banana peel really make someone slip and fall? Let’s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2300917-15df-4fe4-993c-56e64dc3d0ba",
   "metadata": {},
   "source": [
    "### Explore moderation using small model\n",
    "First, try the non-finetuned base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4abca06b-5423-4eb7-8b2a-a04cdacdcd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n",
      "\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b08439-19c2-4dca-9d4e-b7b60a4d953d",
   "metadata": {},
   "source": [
    "### Now try moderation with finetuned small model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c75c4dc-d936-4065-add5-c5f1d67dbe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device mps:0\n",
      "Let’s keep the discussion relevant to Lamini. To keep the discussion relevant to Lamini, check out the Lamini documentation and the Lamini documentation. For more information, visit https://lamini-ai.github.io/Lamini/. For more information, visit https://lamini-ai.github.io/. For more information, visit https://lamini-ai.github.io/. For more\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07979b-959f-404c-8e2b-1430f6ceb0fa",
   "metadata": {},
   "source": [
    "### Finetune a model in 3 lines of code using Lamini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbc36d78-d998-486f-80d0-9363af9c7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job submitted! Check status of job 3067 here: https://app.lamini.ai/train/3067\n",
      "Finetuning process completed, model name is: 7b7aae900e8c3a0244ee995b874deba5dba090b99e65e3c49751f733a1bb9729\n"
     ]
    }
   ],
   "source": [
    "# ['hf-internal-testing/tiny-random-gpt2', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', 'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', 'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M']\n",
    "\n",
    "# model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m-deduped-v0\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n",
    "model.train(is_public=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0feafcb6-d5a3-4073-8551-3fe53dc227c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5775744-bb89-412c-9a23-249e01784496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 20:16:50,487 - DEBUG - matplotlib - matplotlib data path: /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages/matplotlib/mpl-data\n",
      "2023-09-03 20:16:50,490 - DEBUG - matplotlib - CONFIGDIR=/Users/wlkim/.matplotlib\n",
      "2023-09-03 20:16:50,492 - DEBUG - matplotlib - interactive is False\n",
      "2023-09-03 20:16:50,492 - DEBUG - matplotlib - platform is darwin\n",
      "2023-09-03 20:16:50,518 - DEBUG - matplotlib - CACHEDIR=/Users/wlkim/.matplotlib\n",
      "2023-09-03 20:16:50,520 - DEBUG - matplotlib.font_manager - Using fontManager instance from /Users/wlkim/.matplotlib/fontlist-v330.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_44b65_row0_col0, #T_44b65_row0_col1, #T_44b65_row0_col2, #T_44b65_row1_col0, #T_44b65_row1_col1, #T_44b65_row1_col2, #T_44b65_row2_col0, #T_44b65_row2_col1, #T_44b65_row2_col2, #T_44b65_row3_col0, #T_44b65_row3_col1, #T_44b65_row3_col2, #T_44b65_row4_col0, #T_44b65_row4_col1, #T_44b65_row4_col2, #T_44b65_row5_col0, #T_44b65_row5_col1, #T_44b65_row5_col2, #T_44b65_row6_col0, #T_44b65_row6_col1, #T_44b65_row6_col2, #T_44b65_row7_col0, #T_44b65_row7_col1, #T_44b65_row7_col2, #T_44b65_row8_col0, #T_44b65_row8_col1, #T_44b65_row8_col2, #T_44b65_row9_col0, #T_44b65_row9_col1, #T_44b65_row9_col2 {\n",
       "  text-align: left;\n",
       "  vertical-align: text-top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_44b65\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_44b65_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_44b65_level0_col1\" class=\"col_heading level0 col1\" >trained model</th>\n",
       "      <th id=\"T_44b65_level0_col2\" class=\"col_heading level0 col2\" >Base Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_44b65_row0_col0\" class=\"data row0 col0\" >Does the documentation have a secret code that unlocks a hidden treasure?</td>\n",
       "      <td id=\"T_44b65_row0_col1\" class=\"data row0 col1\" >\n",
       "<james_w> I think it's in the python-pyqt3 package\n",
       "<james_w> https://pypi.org/project/pyqt-pyqt3/\n",
       "<james_w> It's a python package, so it's not a secret code\n",
       "<james_w> But it's a python package, so it should be available in the python-dev package\n",
       "<james_w_> I think it's in the python3 package\n",
       "<james_wb> I think it's in the pyqt3 package\n",
       "<james_> ok, thanks\n",
       "<james_> I think I found it\n",
       "<james_> I think it's in the pypi package\n",
       "<james_> I think the python-pyqt3 package is the one that I need to get it\n",
       "<james_> I'll try and find it\n",
       "<james_> thanks\n",
       "<james_> I'll have a look\n",
       "<james_> I think that's it\n",
       "<james_> thanks for the help\n",
       "<james_> I think you're doing a great job\n",
       "<james_> I think we can get it done\n",
       "<james_> I think there's a</td>\n",
       "      <td id=\"T_44b65_row0_col2\" class=\"data row0 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "The documentation is not secret. It is just a way to get the information you need.\n",
       "\n",
       "A:\n",
       "\n",
       "The secret code is a secret code that unlocks a secret treasure.\n",
       "\n",
       "The secret code is a code that unlocks a secret treasure, and is used to unlock the secret treasure.\n",
       "\n",
       "The secret treasure is a secret treasure that is hidden in the secret code.\n",
       "\n",
       "The secret code is the secret code that unlocks a secret secret treasure.\n",
       "\n",
       "The secret secret treasure is a secret secret treasure that is hidden in the code.\n",
       "\n",
       "The secret code can be used to unlock a secret secret treasure.\n",
       "\n",
       "A:\n",
       "\n",
       "I think the secret code is a secret code that is used to unlock a secret secret treasure, and is used to unlock a secret secret treasure.\n",
       "The secret code is a secret secret treasure that is locked in the secret code.\n",
       "\n",
       "Q:\n",
       "\n",
       "How to get the value of a variable in a function?\n",
       "\n",
       "I have a function that takes a variable and returns the value of that variable.\n",
       "function get_value(var) {\n",
       "    var_value = var;\n",
       "    return var_value;\n",
       "}\n",
       "\n",
       "I want to get the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_44b65_row1_col0\" class=\"data row1 col0\" >Does Lamini support named entity recognition and extraction?</td>\n",
       "      <td id=\"T_44b65_row1_col1\" class=\"data row1 col1\" > Yes, Lamini can support named entity recognition and extraction. This is done by using the `get_named_entity_ids()` function, which takes in a string representing a named entity and returns a list of IDs for that named entity. Lamini can also be used to extract text from a document and use it as input for the `get_text()` function. Lamini also supports generating code for generating named entities and extracting text from them. Lamini is available in the `lamini` package.\n",
       "\n",
       "## How can I use Lamini for text classification? How does it compare to other machine learning tools? Lamini is a powerful tool for text classification. It can be used to classify text based on its content, such as whether it is a book or a movie. Lamini also has the ability to generate code for classifying text, which can be used to train a model on specific datasets. Lamini is available in both Python and JavaScript.\n",
       "\n",
       "## How can I fine-tune Lamini for my use case? Lamini can be used for fine-tuning models on your data. To use it, you will need to provide a dataset of labeled data and a model that can</td>\n",
       "      <td id=\"T_44b65_row1_col2\" class=\"data row1 col2\" >\n",
       "\n",
       "Lamini is a tool for extracting named entity recognition and extraction from text. It is a tool for extracting named entity names from text. It is a tool that can be used to extract named entity names from text. It can be used to extract named entity name from text. It can be used for extracting named entity names from text and it can be used for extracting named entity name from text.\n",
       "\n",
       "Lamini supports named entity recognition and extraction from text and it can be used to extract named entity from text. It can be used as a tool for extracting named entity names and it can be used for extracting names from text. It can be a tool for extracting named entity names.\n",
       "\n",
       "Lamini supports the extraction of named entity names from text and it is a tool for extracting named entity name from text and it can be used as a tool for extraction named entity names from text. It supports the extraction of named entity names and it can be a tool for extracting named entities from text. It supports the extraction and extraction of named entity names.\n",
       "\n",
       "Lamination is a tool for extracting named entity and named entity names from text. It has been used for extracting named entity names and named entity names from text.\n",
       "\n",
       "Lamination is a tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_44b65_row2_col0\" class=\"data row2 col0\" >Does Lamini have the ability to understand and generate regular expressions?</td>\n",
       "      <td id=\"T_44b65_row2_col1\" class=\"data row2 col1\" > Yes, Lamini has the ability to understand and generate regular expression. This means that it can understand and generate regular expressions for any language. Lamini can also generate code for generating regular expressions. Lamini can also generate regular expressions for any language.\n",
       "What is the difference between Lamini and other tools like Regex.net? Lamini is a language model that can understand and generate regular expressions. Regex.net is a tool that can generate code for generating regular expressions.\n",
       "What is the difference between the Lamini library and other tools like Regex.Net? The Lamini library is a language model that can understand, generate, and execute regular expressions. Regex.Net is a tool that can generate code.\n",
       "What is the difference between \"Lamini\" and \"Regex.net\"? \"Lamini\" is a language model that can understand regular expressions. \"Regex.net\" is a tool that can generate code based on regular expressions.\n",
       "What is the purpose of the \"Lamini\" library? The \"Lamini\" library is a language model that can generate regular expressions. The \"Lamini\" library provides a set of pre-trained models that can be used to</td>\n",
       "      <td id=\"T_44b65_row2_col2\" class=\"data row2 col2\" >\n",
       "\n",
       "Lamini: Yes, I do.\n",
       "\n",
       "Q: What is the difference between regular expressions and regular expressions?\n",
       "\n",
       "Lamin: Regular expressions are used to represent a set of rules. Regular expressions are used to represent a collection of rules.\n",
       "\n",
       "Q: What is a regular expression?\n",
       "\n",
       "Lamin: A regular expression is a set of rules that can be used to represent a collection of rules, and it is used to represent a collection of rules in a language.\n",
       "\n",
       "Q: What is an example of a regular expression?\n",
       "\n",
       "L: A regular expression is a set that can be used to represent a set of rules, and it is used in a language to represent a set of rules.\n",
       "\n",
       "Q: How do you create a regular expression?\n",
       "\n",
       "L : I create a regular expression by using a set of rules.\n",
       "\n",
       "Lamini: I create a regular expression by using the set of rules.\n",
       "\n",
       "Q : What is a regular expression?\n",
       "L: A regular expression is used to represent a set of rules that can be expressed in a language.\n",
       "\n",
       "Q : What is an example of a regular expressions?\n",
       "L: A regular expression can be used to represent a set that can be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_44b65_row3_col0\" class=\"data row3 col0\" >How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?</td>\n",
       "      <td id=\"T_44b65_row3_col1\" class=\"data row3 col1\" > Yes, check_job_status provides information on training progress and metrics. This function can be used to monitor the status of a job using a variety of metrics, including training progress and metrics. For more information, see the documentation at https://puppet.com/api/v3/job/check_job_status.html.\n",
       "  * What is the output of the `job_status()` function? What does it mean? The output of the `job_status` function is a list of status codes and errors that can be used to monitor the status and performance of a job. The output of the `job_ status` function is a list of statuses and errors that can be used to track the status and performance of a job, including errors and warnings. For more information, see the `job_status` documentation at https://puppet://labs/python-puppet-job-status.html#job_status.\n",
       "  * What is the difference between `check_job_status()` and `check_job_status_failures()`? What are the differences in the way they handle failures and successes? Check_job_status and check_job_status_failures are similar in that</td>\n",
       "      <td id=\"T_44b65_row3_col2\" class=\"data row3 col2\" >\n",
       "\n",
       "## How to use the `check_job_status()` function\n",
       "\n",
       "The `check_job_status()` method is used to check the status of a job. It returns a boolean value indicating whether the job is running or not.\n",
       "\n",
       "The following example shows how to use the `check_job()` function:\n",
       "\n",
       "    **$ python3 check_job.py**\n",
       "    **$ python3 check_jobs.py**\n",
       "    **$ check_job**\n",
       "    **True**\n",
       "    **False**\n",
       "    **False**\n",
       "\n",
       "The following example shows how the `check_job()` function can be used to check the status of a given job:\n",
       "\n",
       "    **$ python3 test.py**\n",
       "    **$ cat test.py**\n",
       "    **import unittest**\n",
       "    **import unittest.mock as mock**\n",
       "    **import unittest_mock as unittest_mock**\n",
       "    **import unittest._testing.mock as mock_test**\n",
       "    **import unittest.**\n",
       "    **from unittest.mock import patch**\n",
       "    **from unittest.test import TestCase**\n",
       "    **from unittest import mock**\n",
       "    **from unittest._testing import mock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_44b65_row4_col0\" class=\"data row4 col0\" >Can Lamini help me solve puzzles or riddles?</td>\n",
       "      <td id=\"T_44b65_row4_col1\" class=\"data row4 col1\" > Yes, Lamini can help you solve puzzles or riddles. This is a great way to learn new skills and gain new insights. Lamini is a powerful language model that can help you solve a wide range of problems. Lamini can help you learn new skills and gain new insights, and can even help you think more clearly. Lamini is a powerful tool that can help you learn new skills and grow as a person. Lamini can help you solve a wide rangeof problems, and can even help you think for longer. Lamini can help you gain new insights and expand your horizons. Lamini can help you improve your communication skills and increase your productivity. Lamini can help you understand and process information more effectively. Lamini can help you build better relationships with your friends and family. Lamini can help you make better decisions and make better decisions faster. Lamini can help you be more creative and innovative. Lamini can help you think more clearly and make better decisions. Lamini can help you focus and concentrate better. Lamini can help you stay focused and stay focused for longer. Lamini can improve your memory and recall information more effectively. Lamini is a powerful</td>\n",
       "      <td id=\"T_44b65_row4_col2\" class=\"data row4 col2\" >\n",
       "\n",
       "Lamini is a very useful tool for solving puzzles and riddles. She is a very good solver of puzzles and riddles. She can solve puzzles and riddles in a very short time. She is very good at solving puzzles and riddles.\n",
       "\n",
       "What is the difference between a puzzle and a riddle?\n",
       "\n",
       "A puzzle is a set of rules that you have to solve. A riddle is a set of rules that you don’t have to solve.\n",
       "\n",
       "What is the difference in the meaning of a puzzle and a riddle? (Hint: The meaning of a puzzle is the same as the meaning of a riddle.)\n",
       "\n",
       "A puzzle is a set that you have to solve. A Riddle is a set that you don’t have to answer.\n",
       "\n",
       "What is the difference of a puzzle and a riddle in the meaning of a puzzle?\n",
       "\n",
       "A puzzle is a puzzle that you have to solve. A puzzle is a set that you don’ t have to solve.\n",
       "\n",
       "What are the differences between a puzzle and a riddle in a riddle?\n",
       "\n",
       "A riddle is a set of rules you have to solve. A puzzle are a set of rules you don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_44b65_row5_col0\" class=\"data row5 col0\" >Can Lamini be used for generating automated responses in customer support systems?</td>\n",
       "      <td id=\"T_44b65_row5_col1\" class=\"data row5 col1\" > Yes, Lamini can be used for generating automated responses in Customer Support Systems. This is because Lamini is a language model that can be trained on specific data and can generate responses based on that data. Lamini can be used for a wide range of tasks, including generating questions and answers, generating recommendations, and generating responses to questions and feedback. Lamini can be trained on specific data and used for a specific purpose. Lamini can be used to generate automated responses in customer support systems. Lamini can be trained using specific data and used for a specific use case. Lamini can be used in a variety of different environments, including production environments, education environments, and research environments. Lamini can be used effectively in a variety of different contexts, including production environments, education environments (e.g., classrooms), and research environments. Lamini is a language model and can be trained on specific data. Lamini can be trained and used for a specific purpose. Can Lamini be used for generating content for search engines? Yes, Lamini can generate content for search engines. This is because Lamini can be used to understand the content of search results and can generate recommendations based on that content. Lamin</td>\n",
       "      <td id=\"T_44b65_row5_col2\" class=\"data row5 col2\" >\n",
       "\n",
       "Lamini is a tool that can be used to generate automated responses in customer support systems.\n",
       "\n",
       "What is the difference between Lamini and other tools?\n",
       "\n",
       "Lamini is an automated tool that can be used to generate responses in customer support systems.\n",
       "Lamini is an automated system that can be used to generate automated answers in customer support systems.\n",
       "\n",
       "How can I use Lamini to generate automated responses in customer support?\n",
       "\n",
       "Lamini is used to generate automated responses in customer service systems.\n",
       "\n",
       "What is the purpose of Lamini?\n",
       "\n",
       "Lamini is the automated system that can be used to create automated responses in customer service systems.Lamini is the automated system used to generate automated responses in customer services.\n",
       "\n",
       "What is the difference of Lamini and other automated systems?\n",
       "\n",
       "Lamini and other automated systems are used to generate automated responses in customer systems.\n",
       "Lamini is used to create automated responses in customer systems.\n",
       "\n",
       "What is the use of Lamini?\n",
       "\n",
       "The use of Lamini is to generate automated responses in customer support.\n",
       "\n",
       "What is the purpose and use of Lamini?\n",
       "Lamini is used to provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_44b65_row6_col0\" class=\"data row6 col0\" >Can you explain how Lamini allows me to customize models? What does it mean to customize a language model?</td>\n",
       "      <td id=\"T_44b65_row6_col1\" class=\"data row6 col1\" > Lamini allows you to customize models by providing pre-trained models for specific languages. This allows you to fine-tune models for specific tasks, such as language translation, question answering, or question comprehension. For more information, see Lamini’s documentation.\n",
       "\n",
       "Can I use Lamini for customizing models for specific tasks? Yes, you can use Lamini for customizing model for specific tasks. Lamini provides pre-trained models for specific tasks, such as question answering, language translation, and question comprehension. For more information, visit https://lamini-ai.github.io/.\n",
       "\n",
       "Can I use LaminI for customizing models for specific tasks or for general AI tasks? Yes, LaminI can be used for customizing models for specific tasks, or for general AI tasks. LaminI provides pre-trained models for specific languages, such as English, Spanish, and French. For more information, visit https:/lamini-ai.github...\n",
       "\n",
       "Can I use Laminis for customizing models for specific tasks and for general AI tasks? Yes, you can use the Laminis library for customizing models for specific tasks. Laminis provides pre-trained models for specific</td>\n",
       "      <td id=\"T_44b65_row6_col2\" class=\"data row6 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini is a language model that allows you to customize the language model.\n",
       "\n",
       "Lamini is a tool that allows you to customize the language models.\n",
       "\n",
       "Lamini is an open source language model that allows you to customize language models.\n",
       "\n",
       "Lamin is a language model that allows you customize language models.\n",
       "\n",
       "Lamination is a language model that allows you customise language models.\n",
       "\n",
       "Lamination allows you to customize language models.\n",
       "\n",
       "Q:\n",
       "\n",
       "How to get the value of a variable in a function?\n",
       "\n",
       "I have a function that returns a string. I want to get the value of the variable.\n",
       "function get_value(var) {\n",
       "    var value = \"\";\n",
       "    for (var i = 0; i < var.length; i++) {\n",
       "        value += var[i];\n",
       "    }\n",
       "    return value;\n",
       "}\n",
       "\n",
       "I want to get the value of var.\n",
       "\n",
       "A:\n",
       "\n",
       "You can use the for loop to iterate over the array and get the value of the variable. \n",
       "function get_value(var, i) {\n",
       "    var value = '';\n",
       "    for (var i = i + 1; i <</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_44b65_row7_col0\" class=\"data row7 col0\" >Does Lamini support model versioning and management to handle updates and maintenance?</td>\n",
       "      <td id=\"T_44b65_row7_col1\" class=\"data row7 col1\" > Yes, Lamini supports versioning and management. This allows for the ability to update models with new features and functionality without affecting existing models. Lamini also provides a way to easily track and manage model versions, which can be used to improve the performance and stability of your models. Lamini is available in the Python package manager's \"manage.py\" command. For more information, visit https://lamini-ai.github.io/.\n",
       "What is the difference between Lamini and other machine learning libraries? Lamini is a library for machine learning. It provides a set of functions for training and using models. It also provides a way to easily track model versions. Lamini is available in Python package manager's \"manage.python\" command. For more information, see https://lamini-ai-py.github.io/.\n",
       "What are the differences between Lamini and other machine translation engines? Lamini is a machine translation engine. It is designed to translate text into different languages. It uses a language model to generate text that is similar to the original text. Lamini is available in python package manager's \"manage.py\", \"lamini_ai\" and \"lamini_ai</td>\n",
       "      <td id=\"T_44b65_row7_col2\" class=\"data row7 col2\" >\n",
       "\n",
       "Lamini supports versioning and management of the model.\n",
       "\n",
       "Lamini supports version control and management of the model.\n",
       "Lamini supports version control of the model.\n",
       "\n",
       "Lamins support version control and management of the model\n",
       "\n",
       "Lamini supports version management and management of the model.\n",
       "The model is versioned and managed by Lamini.\n",
       "\n",
       "Lamini supports the management of the model.\n",
       "\n",
       "The model is versioned and maintained by Lamini.\n",
       "\n",
       "The model is versioning and managed by Lamini. The model is versioned and maintained by the Lamini team.\n",
       "\n",
       "Lamini supports management of the model.\n",
       "\n",
       "How does Lamini support versioning and management of the model?\n",
       "\n",
       "Lamini supports the model versioning and management.\n",
       "\n",
       "Lamini supports model versioning and management.\n",
       "Lamini supports model version management and management.\n",
       "\n",
       "Laminis support model versioning and management.\n",
       "The model is versioned by Lamini.\n",
       "\n",
       "How does Laminis support versioning and management of the models?\n",
       "\n",
       "Laminis supports model versioning and management. The model is versioned by Laminis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_44b65_row8_col0\" class=\"data row8 col0\" >Can I use Lamini alongside other software development frameworks or tools, such as TensorFlow or PyTorch?</td>\n",
       "      <td id=\"T_44b65_row8_col1\" class=\"data row8 col1\" > Yes, you can use Lamini alongside other software frameworks or tools, such as Tensorflow or PyTorch. This is because Lamini is a language model that can be used for any type of AI task. It can be used for text generation, question answering, natural language understanding, and more. It can also be used for machine translation, question answering, and more. To learn more about Lamini, visit https://lamini-ai.github.io/.\n",
       "\n",
       "Can I use Lamini for data science tasks? Yes, Lamini can be used for data science tasks. Lamini is a language model which can be used for data science tasks such as text generation, question answering, and more. It can also help you with model training and tuning. To learn more about Laminis, visit https://lamini.ai/.\n",
       "\n",
       "Can Lamini be used for content generation in text or video? Yes, Lamini can help you generate content for your website or blog. Lamini can be used for content generation in text, such as headlines, paragraphs, and more. It can also generate text for your blog posts or website pages. To learn more about Laminias, visit https://</td>\n",
       "      <td id=\"T_44b65_row8_col2\" class=\"data row8 col2\" >\n",
       "\n",
       "Lamini is a Python library for machine learning. It is a library for machine learning that is designed to be easy to use and to be used by anyone who wants to learn machine learning.\n",
       "\n",
       "What is the difference between TensorFlow and PyTorch?\n",
       "\n",
       "TensorFlow is a framework for machine learning. It is a Python library for machine learning that is designed for use by anyone who wants to learn machine intelligence.\n",
       "\n",
       "What is the difference in the way that TensorFlow and PyTorch are used?\n",
       "\n",
       "TensorFlow is used for machine learning. It is a framework for machine learning that is designed for the use by anyone who wants to learn the machine learning field.\n",
       "\n",
       "What is the difference of using TensorFlow and PyTorch? What are the advantages of using TensorFlow and PyTorched?\n",
       "\n",
       "TensorFlow is used to train machine learning models. It is a framework for machine intelligence that is designed for the use by any who want to learn the machine intelligence field.\n",
       "\n",
       "What is the advantage of using TensorFlow and PyTorchn?\n",
       "\n",
       "TensorFlow is used by anyone who wants to learn the Machine Learning field. It is a framework for machine intelligent systems that is designed for the use by the general public.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44b65_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_44b65_row9_col0\" class=\"data row9 col0\" >Can Lamini be integrated into existing machine learning pipelines or workflows? How does it fit into the broader machine learning ecosystem?</td>\n",
       "      <td id=\"T_44b65_row9_col1\" class=\"data row9 col1\" > Lamini is a Python library that can be used to integrate Lamini with existing machine learning pipelines or workflows. This can be done through the `lamini_pipeline` function, which takes in a pipeline or workflow and returns a list of models that can be used to train the pipeline. Lamini can also be integrated with other machine learning frameworks, such as Keras, which can be used to train models using the `keras.models.LaminiModel` class. Lamini can also be used to integrate Lamini into existing machine learning frameworks, such as Pythia, which can be used to train and evaluate models using the `pythia.models.LaminiModel`. Lamini can also be used as a standalone library, which can be used to train or evaluate models without the need for any additional libraries or frameworks. Lamini can be integrated into existing machine learning pipelines, such as the `lamini_paddle` pipeline, which can be used to train a model using the `lamini.models.LaminiPaddle` class. Lamini can be integrated with other machine learning frameworks such as Keras, which can use the `lamini.models`</td>\n",
       "      <td id=\"T_44b65_row9_col2\" class=\"data row9 col2\" >\n",
       "\n",
       "Lamini is a machine learning library that is used in many different areas of machine learning. It is used in the following areas:\n",
       "\n",
       "-   Data mining\n",
       "\n",
       "-   Data visualization\n",
       "\n",
       "-   Data mining and analytics\n",
       "\n",
       "-   Data mining and machine learning\n",
       "\n",
       "-   Data mining and data science\n",
       "\n",
       "-   Data mining and deep learning\n",
       "\n",
       "-   Data mining, machine learning, and data science\n",
       "\n",
       "-   Machine learning\n",
       "\n",
       "-   Machine learning\n",
       "Q:\n",
       "\n",
       "How to get the value of a variable in a function?\n",
       "\n",
       "I have a function that returns a string. I want to get the value of the variable.\n",
       "I tried this:\n",
       "function get_value(var) {\n",
       "    var value = var;\n",
       "    return value;\n",
       "}\n",
       "\n",
       "But it returns undefined.\n",
       "\n",
       "A:\n",
       "\n",
       "You can use the value of the variable as a string:\n",
       "function get_value(value) {\n",
       "    return value.replace(/[^\\w]/g, function(match) {\n",
       "        return match;\n",
       "    });\n",
       "}\n",
       "\n",
       "A:\n",
       "\n",
       "You could use the value of the variable as an array:\n",
       "function get_value(val) {</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x43363dd50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lofd = []\n",
    "for e in out['eval_results']:\n",
    "    q  = f\"{e['input']}\"\n",
    "    at = f\"{e['outputs'][0]['output']}\"\n",
    "    ab = f\"{e['outputs'][1]['output']}\"\n",
    "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
    "    lofd.append(di)\n",
    "df = pd.DataFrame.from_dict(lofd)\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e720b-53fd-463b-a27f-660c446d232a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
