{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b101eb4-4503-4b2f-ba86-facbc4797577",
   "metadata": {},
   "source": [
    "# 언어 모델 미세 조정하기\n",
    "\n",
    "<!--- @wandbcode{dlai_05} -->\n",
    "\n",
    "언어 모델을 미세 조정하여 wandb와 통합된 HuggingFace 트레이너를 사용하여 캐릭터 배경 스토리를 생성하는 방법을 살펴 보겠습니다. 여기서는 리소스 제약으로 인해 작은 언어 모델(`TinyStories-33M`)을 사용하지만 여기서 배운 내용은 대규모 모델에도 적용할 수 있을 것입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55572e6-a606-400d-ba4b-ece76e91350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: psutil in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: pyyaml in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "!pip install accelerate -U\n",
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30711d69-66e1-44f2-87cf-44346e256f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "transformers.set_seed(42)\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a536f025-5cc8-437e-ba8b-1c19de330786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimwooglae\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be744469-e50d-4328-8046-371c2a3a8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roneneldan/TinyStories-33M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def3405-3752-4e1f-b032-b178df23c509",
   "metadata": {},
   "source": [
    "### 데이터 준비\n",
    "\n",
    "먼저 허깅페이스에서 던전 앤 드래곤 캐릭터의 약력이 포함된 데이터 세트를 로드하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58629c07-022d-43bc-83d9-82cf1ff40233",
   "metadata": {},
   "source": [
    "> You can expect to get some warning here, this is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2be530-40f4-4572-8ccb-34a2c2943a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('MohamedRashad/characters_backstories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46106696-cc34-4eed-bba7-31118263379d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Generate Backstory based on following information\\nCharacter Name: Dewin \\nCharacter Race: Halfling\\nCharacter Class: Sorcerer bard\\n\\nOutput:\\n',\n",
       " 'target': 'Dewin thought he was a wizard, but it turned out it was the draconic blood in his veins that brought him eldritch power.  Music classes in wizarding college taught him yet another use for his power, and when he was expelled he took up adventuring'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at one example\n",
    "ds[\"train\"][400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ce8717-2474-44a6-b193-d2f680e33613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this dataset has no validation split, we will create one\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654d2c20-02ab-4cf4-8fda-b8316483475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a tokenizer from model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "\n",
    "# We'll need padding to have same length sequences in a batch\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a tokenization function that first concatenates text and target\n",
    "def tokenize_function(example):\n",
    "    merged = example[\"text\"] + \" \" + example[\"target\"]\n",
    "    batch = tokenizer(merged, padding='max_length', truncation=True, max_length=128)\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "# Apply it on our dataset, and remove the text columns\n",
    "tokenized_datasets = ds.map(tokenize_function, remove_columns=[\"text\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf55582-0041-46ca-a87f-7f1cd9ea5fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Backstory based on following information\n",
      "Character Name: Mr. Gale\n",
      "Character Race: Half-orc\n",
      "Character Class: Cleric\n",
      "\n",
      "Output:\n",
      " Growing up the only half-orc in a small rural town was rough. His mother didn't survive childbirth and so was raised in a church in a high mountain pass, his attention was always drawn by airships passing through, and dreams of an escape. Leaving to strike out on his own as early as he could he made a living for most of his life as an airship sailor, and occasionally a pirate. A single storm visits him throughout his life, marking every major\n"
     ]
    }
   ],
   "source": [
    "# Let's check out one prepared example\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][900]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4cf11a-9748-4cf4-8f92-3026fb9a25f1",
   "metadata": {},
   "source": [
    "### 학습\n",
    "\n",
    "HF Transformers와 wandb 통합을 사용하여 데이터 세트에서 사전 학습된 언어 모델을 미세 조정해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7ea3946-3254-47e2-bded-7ba6c3afa826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train a causal (autoregressive) language model from a pretrained checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e85e75-bd58-4a23-9d73-827eba679813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/wlkim/github/DeepLearning.AI/Evaluating and Debugging Generative AI/wandb/run-20230806_211327-te6hn1rj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimwooglae/dlai_lm_tuning/runs/te6hn1rj' target=\"_blank\">still-serenity-6</a></strong> to <a href='https://wandb.ai/kimwooglae/dlai_lm_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimwooglae/dlai_lm_tuning' target=\"_blank\">https://wandb.ai/kimwooglae/dlai_lm_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimwooglae/dlai_lm_tuning/runs/te6hn1rj' target=\"_blank\">https://wandb.ai/kimwooglae/dlai_lm_tuning/runs/te6hn1rj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start a new wandb run\n",
    "run = wandb.init(project='dlai_lm_tuning', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2291453-a9b9-412e-bb02-42c954d2bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-characters-backstories\",\n",
    "    report_to=\"wandb\", # we need one line to track experiments in wandb\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True, # force cpu use, will be renamed `use_cpu`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aaab664-497c-4809-a54d-5e37b905b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use HF Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4457ec-2eb2-4e09-826f-b3c0497861e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wlkim/anaconda3/envs/deeplearning-ai/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='233' max='233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [233/233 02:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.942100</td>\n",
       "      <td>3.345123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=233, training_loss=3.7518335007802612, metrics={'train_runtime': 136.5179, 'train_samples_per_second': 13.603, 'train_steps_per_second': 1.707, 'total_flos': 40423258718208.0, 'train_loss': 3.7518335007802612, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1138f09-7104-450b-b9a7-0b4ba1679556",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error() # suppress tokenizer warnings\n",
    "\n",
    "prefix = \"Generate Backstory based on following information Character Name: \"\n",
    "\n",
    "prompts = [\n",
    "    \"Frogger Character Race: Aarakocra Character Class: Ranger Output: \",\n",
    "    \"Smarty Character Race: Aasimar Character Class: Cleric Output: \",\n",
    "    \"Volcano Character Race: Android Character Class: Paladin Output: \",\n",
    "]\n",
    "\n",
    "table = wandb.Table(columns=[\"prompt\", \"generation\"])\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prefix + prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, do_sample=True, max_new_tokens=50, top_p=0.9)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    table.add_data(prefix + prompt, output_text)\n",
    "    \n",
    "wandb.log({'tiny_generations': table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85352fc8-d93e-4f27-aa3b-4b626991f01c",
   "metadata": {},
   "source": [
    "**Note**: LLM이 항상 동일한 결과를 생성하는 것은 아닙니다. 생성된 캐릭터와 배경 스토리는 동영상과 다를 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a31fb13-0c9f-41ba-aee5-3cae9423fc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b19e0a3f70442180f8824e387bec20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▂▃▄▁▂▁▂▂▂▂▃▃▂▂▂▁▂▂▂▂▃▂▂▂▁▂▂▃▁▁▁▁▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.34512</td></tr><tr><td>eval/runtime</td><td>12.3185</td></tr><tr><td>eval/samples_per_second</td><td>37.748</td></tr><tr><td>eval/steps_per_second</td><td>4.79</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>233</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.9421</td></tr><tr><td>train/total_flos</td><td>40423258718208.0</td></tr><tr><td>train/train_loss</td><td>3.75183</td></tr><tr><td>train/train_runtime</td><td>160.1518</td></tr><tr><td>train/train_samples_per_second</td><td>11.595</td></tr><tr><td>train/train_steps_per_second</td><td>1.455</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-shadow-3</strong> at: <a href='https://wandb.ai/kimwooglae/dlai_lm_tuning/runs/9i7hvi8h' target=\"_blank\">https://wandb.ai/kimwooglae/dlai_lm_tuning/runs/9i7hvi8h</a><br/> View job at <a href='https://wandb.ai/kimwooglae/dlai_lm_tuning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4MjcxNjg0/version_details/v1' target=\"_blank\">https://wandb.ai/kimwooglae/dlai_lm_tuning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4MjcxNjg0/version_details/v1</a><br/>Synced 6 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230806_205745-9i7hvi8h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afacea-7058-41ad-857d-7a518c7ccf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90447814-8029-48df-bef1-cce3c6483eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad72d26-dbcf-4271-9389-37d44f83dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90479665-20d0-4624-99a0-d89e975cbf65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6234d-cf6a-4219-a395-f09527f726e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a021313-08ee-4b89-a000-3bf789017eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2231ab-fe04-4be0-82d4-cf6d4c5ee5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
